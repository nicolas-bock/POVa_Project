{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POVa - Facial recognition from RGB-Depth images (identification)\n",
    "\n",
    "Our goal is to get some face recognition working using RGB-D data (e.g. Kinect).\n",
    "\n",
    "- Detect faces in images using an existing detector. Good choices are OpenCV, Dlib or MTCNN https://github.com/DCGM/mtcnn.\n",
    "- Align the face based on detected facial features (map to an average face).\n",
    "- Optional: (Try align 3D face pose using the depth data)\n",
    "- Train a neural network to identify faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "dataset_name = \"atulanandjha/lfwpeople\"\n",
    "DATA_PATH = \"../data/\" + dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run only if you don't have the dataset already in your project.\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(dataset_name)\n",
    "print(\"\\n\"+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p $DATA_PATH\n",
    "%mv $path/* $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf $DATA_PATH/*.tgz -C $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if not os.path.isdir(\"../data/dataset/\"):\n",
    "    os.mkdir(\"../data/dataset/\")\n",
    "    os.mkdir(\"../data/dataset/training\")\n",
    "    os.mkdir(\"../data/dataset/test\")\n",
    "\n",
    "for base_path, dirs, _ in os.walk(\"../data/\" + dataset_name + \"/lfw_funneled\"):\n",
    "    for dir in dirs:\n",
    "        if len(os.listdir(os.path.join(base_path, dir))) >= 5:\n",
    "            if not os.path.isdir(os.path.join(\"../data/dataset/training\", dir)):\n",
    "                os.mkdir(os.path.join(\"../data/dataset/training\", dir))\n",
    "    \n",
    "            file_list = os.listdir(os.path.join(base_path, dir))\n",
    "            random.shuffle(file_list)\n",
    "    \n",
    "            split = int(0.8 * len(file_list))\n",
    "    \n",
    "            for file_name in file_list[0:split]:\n",
    "                full_img_path = os.path.join(base_path, dir, file_name)\n",
    "                if os.path.isfile(full_img_path):\n",
    "                    shutil.copy(full_img_path, os.path.join(\"../data/dataset/training\", dir))\n",
    "        \n",
    "            for file_name in file_list[split:]:\n",
    "                full_img_path = os.path.join(base_path, dir, file_name)\n",
    "                if os.path.isfile(full_img_path):\n",
    "                    shutil.copy(full_img_path, \"../data/dataset/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFW - People (Face Recognition) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mtcnn tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "train_dataset_path = \"../data/dataset/training/\"\n",
    "test_dataset_path = \"../data/dataset/test/\"\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "image_files = []\n",
    "for root, _, files in os.walk(train_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            image_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images = random.sample(image_files, 10)\n",
    "\n",
    "image_size = (200, 200)\n",
    "rows, cols = 2, 5\n",
    "canvas = np.zeros((rows * image_size[1], cols * image_size[0], 3), dtype=np.uint8)\n",
    "\n",
    "for idx, image_path in enumerate(random_images):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not load image: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    faces = detector.detect_faces(image)\n",
    "    for face in faces:\n",
    "        box = face['box']\n",
    "        cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (255, 0, 0), 2)\n",
    "\n",
    "    image_resized = cv2.resize(image, image_size)\n",
    "\n",
    "    row = idx // cols\n",
    "    col = idx % cols\n",
    "    y_start = row * image_size[1]\n",
    "    y_end = y_start + image_size[1]\n",
    "    x_start = col * image_size[0]\n",
    "    x_end = x_start + image_size[0]\n",
    "\n",
    "    canvas[y_start:y_end, x_start:x_end, :] = image_resized\n",
    "\n",
    "cv2.imshow(\"Detected Faces Collage\", canvas)\n",
    "cv2.waitKey(0)  # Escape key\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the number of different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 0\n",
    "for root, dirs, files in os.walk(DATA_PATH + \"/lfw_funneled/\"):\n",
    "    n_labels += len(dirs)\n",
    "    break\n",
    "\n",
    "print(\"Number of classes/directories in the dataset: \", n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=20, resize=1.0)\n",
    "X = lfw_people.images\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "n_samples, W, H = lfw_people.images.shape\n",
    "n_features = X.shape[1]\n",
    "n_classes = target_names.shape[0]\n",
    "n_labels = n_classes\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X:\", X)\n",
    "print(\"\\ny:\", y)\n",
    "\n",
    "print(\"\\nX shape:\", X.shape)\n",
    "print(\"\\ny shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1)\n",
    "figs, axes = plt.subplots(4, 6)\n",
    "for i in range(4):\n",
    "    for j in range(6): \n",
    "        axes[i, j].imshow(X[i*6+j,:,:], cmap='gray')\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# normalization\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype( 'float32') / 255.0\n",
    "\n",
    "# categorical vectors\n",
    "y_train = to_categorical(y_train, n_labels)\n",
    "y_test  = to_categorical(y_test, n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Feature Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_guide_lines(image, keypoints):\n",
    "    left_eye = keypoints['left_eye']\n",
    "    right_eye = keypoints['right_eye']\n",
    "    \n",
    "    eye_center = (\n",
    "        int((left_eye[0] + right_eye[0]) / 2),\n",
    "        int((left_eye[1] + right_eye[1]) / 2)\n",
    "    )\n",
    "    \n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    angle = np.arctan2(dy, dx)\n",
    "    angle_degrees = np.degrees(angle)\n",
    "\n",
    "    length = 200\n",
    "    x1 = int(eye_center[0] - length * np.cos(angle))\n",
    "    y1 = int(eye_center[1] - length * np.sin(angle))\n",
    "    x2 = int(eye_center[0] + length * np.cos(angle))\n",
    "    y2 = int(eye_center[1] + length * np.sin(angle))\n",
    "\n",
    "    cv2.line(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    perp_angle = angle + np.pi / 2  # Ajout de 90° en radians\n",
    "    x3 = int(eye_center[0] - length * np.cos(perp_angle))\n",
    "    y3 = int(eye_center[1] - length * np.sin(perp_angle))\n",
    "    x4 = int(eye_center[0] + length * np.cos(perp_angle))\n",
    "    y4 = int(eye_center[1] + length * np.sin(perp_angle))\n",
    "\n",
    "    cv2.line(image, (x3, y3), (x4, y4), (255, 0, 0), 2)\n",
    "\n",
    "    return image, angle_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face(image, keypoints, output_size=(128, 128)):\n",
    "    left_eye = keypoints['left_eye']\n",
    "    right_eye = keypoints['right_eye']\n",
    "\n",
    "    # Center point between the eyes\n",
    "    eye_center = (\n",
    "        int((left_eye[0] + right_eye[0]) / 2),\n",
    "        int((left_eye[1] + right_eye[1]) / 2)\n",
    "    )\n",
    "    \n",
    "    # Angle between the eyes\n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dy, dx))\n",
    "\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(eye_center, angle, 1.0)\n",
    "    aligned_image = cv2.warpAffine(\n",
    "        image, rotation_matrix, (image.shape[1], image.shape[0]),\n",
    "        flags=cv2.INTER_CUBIC\n",
    "    )\n",
    "\n",
    "    return aligned_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "\n",
    "random_images = random.sample(image_files, 3)\n",
    "for idx, image_path in enumerate(random_images):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not load image: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    faces = detector.detect_faces(image)\n",
    "    if len(faces) == 0:\n",
    "        print(f\"No faces detected in image: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    face = faces[0]\n",
    "    box = face['box']\n",
    "    keypoints = face['keypoints']\n",
    "\n",
    "    image_with_lines, angle = draw_guide_lines(image.copy(), keypoints)\n",
    "\n",
    "    cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (255, 0, 0), 2)\n",
    "\n",
    "    x_, y_, width_, height_ = box\n",
    "    face_region = image[y_:y_ + height_, x_:x_ + width_]\n",
    "    aligned_face = align_face(face_region, keypoints)\n",
    "\n",
    "    axes[idx, 0].imshow(cv2.cvtColor(image_with_lines , cv2.COLOR_BGR2RGB))\n",
    "    axes[idx, 0].set_title(f\"Original Image (angle: {angle:.2f}°)\")\n",
    "    axes[idx, 1].imshow(cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB))\n",
    "    axes[idx, 1].set_title(\"Original Face\")\n",
    "    axes[idx, 2].imshow(cv2.cvtColor(aligned_face, cv2.COLOR_BGR2RGB))\n",
    "    axes[idx, 2].set_title(\"Aligned Face\")\n",
    "    \n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create(25)\n",
    "\n",
    "img = x_train[0]\n",
    "\n",
    "# Rescale the values to [0, 1]\n",
    "normalized_img = (img - img.min()) / (img.max() - img.min())\n",
    "scaled_img = (normalized_img * 255).astype('uint8')\n",
    "\n",
    "key_desc = sift.detectAndCompute(scaled_img, None)\n",
    "image = cv2.drawKeypoints(scaled_img, key_desc[0], None)\n",
    "\n",
    "figure, ax = plt.subplots(1, 2, figsize=(6, 6))\n",
    "ax[0].imshow(img, cmap='gray')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[1].imshow(image)\n",
    "ax[1].set_title(\"SIFT Keypoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = []\n",
    "for root, _, files in os.walk(train_dataset_path):\n",
    "    for file in files:\n",
    "        if file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            image_files.append(os.path.join(root, file))\n",
    "\n",
    "random_images = random.sample(image_files, 10)\n",
    "\n",
    "figure, axes = plt.subplots(2, 5, figsize=(10, 8))\n",
    "figure.suptitle(\"Facial Keypoints Detection\")\n",
    "\n",
    "row = 0\n",
    "for idx, image_path in enumerate(random_images):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not load image: {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Uncomment to get the path of the image\n",
    "    # print(f\"Processing image: {image_path}\")\n",
    "    \n",
    "    faces = detector.detect_faces(image)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(f\"No faces detected in the image {idx}\")\n",
    "        continue\n",
    "\n",
    "    # print(f\"Found {len(faces)} face(s) in the image.\")\n",
    "    \n",
    "    for face_idx, face in enumerate(faces):\n",
    "        keypoints = face['keypoints']\n",
    "        \n",
    "        # Uncomment to print the keypoints + the position of the facial features\n",
    "        # print(f\"Face {face_idx + 1} keypoints:\")\n",
    "        # for key, point in keypoints.items():\n",
    "        #    print(f\"{key}: ({int(point[0])}, {int(point[1])})\")\n",
    "\n",
    "        for key, point in keypoints.items():\n",
    "            cv2.circle(image, point, 2, (0, 255, 255), -1)\n",
    "\n",
    "        box = face['box']\n",
    "        cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (255, 0, 0), 2)\n",
    "    \n",
    "    if idx % 5 == 0 and idx != 0:\n",
    "        row += 1\n",
    "\n",
    "    axes[row, idx%5].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[row, idx%5].set_title(f\"{len(faces)} face(s)\")\n",
    "    axes[row, idx%5].set_xticks([])\n",
    "    axes[row, idx%5].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_aligned_path = \"../data/dataset/training_aligned\"\n",
    "test_aligned_path = \"../data/dataset/test_aligned\"\n",
    "\n",
    "def create_aligned_dataset(input_path, output_path):\n",
    "    for root, _, files in os.walk(input_path):\n",
    "        for file in files:\n",
    "            if file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image = cv2.imread(image_path)\n",
    "\n",
    "                faces = detector.detect_faces(image)\n",
    "                for idx, face in enumerate(faces):\n",
    "                    box = face['box']\n",
    "                    keypoints = face['keypoints']\n",
    "                    \n",
    "                    x_, y_, width_, height_ = box\n",
    "                    face_region = image[y_:y_ + height_, x_:x_ + width_]\n",
    "                    \n",
    "                    aligned_face = align_face(face_region, keypoints)\n",
    "                    \n",
    "                    aligned_image_path = os.path.join(output_path, f\"{os.path.basename(file).split('.')[0]}_aligned_{idx}.jpg\")\n",
    "                    cv2.imwrite(aligned_image_path, aligned_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(training_aligned_path):\n",
    "    os.makedirs(training_aligned_path)\n",
    "    create_aligned_dataset(\"../data/dataset/training\", training_aligned_path)\n",
    "\n",
    "if not os.path.exists(test_aligned_path):\n",
    "    os.makedirs(test_aligned_path)\n",
    "    create_aligned_dataset(\"../data/dataset/test\", test_aligned_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs     = 100\n",
    "l_rate      = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def baseline(W=32, H=32, nclass=10, nchannel=3,lr=1e-4):\n",
    "    in1 = layers.Input(shape=(W, H, nchannel))\n",
    "    x = layers.Conv2D(32, (3, 3), strides=(1, 1),\n",
    "                      padding='valid',\n",
    "                      activation='relu')(in1)    \n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x = layers.BatchNormalization()(x)    \n",
    "    x = layers.Conv2D(32, (3, 3), strides=(1, 1),\n",
    "                      padding='valid',\n",
    "                      activation='relu')(x)    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)    \n",
    "    x = layers.Conv2D(64, (3, 3), strides=(1, 1),\n",
    "                      padding='valid',\n",
    "                      activation='relu')(x)\n",
    "    x = layers.MaxPool2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(1280, activation='relu')(x)\n",
    "    output = layers.Dense(nclass, activation='softmax')(x)\n",
    "    model = Model(inputs=in1, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './checkpoints/checkpoint.weights.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = baseline(W=W, H=H, nclass=n_labels, nchannel=1, lr=l_rate)\n",
    "model_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_base.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[model_checkpoint_callback], validation_split=0.2, verbose=False)\n",
    "\n",
    "model_base.load_weights(checkpoint_filepath)   \n",
    "test_loss, test_acc = model_base.evaluate(x_test, y_test)    \n",
    "print('test acc for model_base: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss'] \n",
    "\n",
    "plt.figure(2)\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc - baseline')\n",
    "plt.plot(epochs, val_acc, 'm:', label='Validation acc - baseline')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.plot(epochs, loss, 'r',  label='Training loss - baseline')\n",
    "plt.plot(epochs, val_loss, 'm:', label='Validation loss - baseline')\n",
    "plt.title( 'Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
